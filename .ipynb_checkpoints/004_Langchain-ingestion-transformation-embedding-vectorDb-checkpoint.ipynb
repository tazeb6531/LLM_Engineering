{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9b8fb3",
   "metadata": {},
   "source": [
    "# GenAI API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690f087",
   "metadata": {},
   "source": [
    "## 1. langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52a207",
   "metadata": {},
   "source": [
    "### Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b36bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "## Data Ingestging\n",
    "import bs4\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader, ArxivLoader, WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f66f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = TextLoader('speech.txt').load()\n",
    "docs_pdf = PyPDFLoader('attention.pdf').load()\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     )))\n",
    "docs_ark = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()\n",
    "docs_wk = WikipediaLoader(query=\"Generative AI\", load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337ed711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_documents\n",
    "# docs_pdf\n",
    "# len(docs_ark)\n",
    "# print(docs_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4cd5e",
   "metadata": {},
   "source": [
    "### 2. Data Transformation\n",
    "\n",
    "**Text Splitting from Documents- RecursiveCharacter Text Splitters**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "- **How the text is split:** by list of characters.\n",
    "- **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6f777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "## Data transformation/Chunk\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fbf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "speech=\"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech=f.read()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "text=text_splitter.create_documents([speech])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82410342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_documents[0])\n",
    "# print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac59a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\",  chunk_size=100, chunk_overlap=20)\n",
    "final_documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5427bf",
   "metadata": {},
   "source": [
    "**How to split by HTML header**\n",
    "\n",
    "**How to split JSON data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d90c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13169de5",
   "metadata": {},
   "source": [
    "### 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e85e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# OpenaAi embedding \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Ollama embedding\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Huggingface embedding\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2cc0e",
   "metadata": {},
   "source": [
    "**1. OpenAI Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1024=OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024)\n",
    "\n",
    "# Example\n",
    "text=\"This is a tutorial on OPENAI embedding\"\n",
    "query_result=embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f1702",
   "metadata": {},
   "source": [
    "**2. Ollama Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ollama=(OllamaEmbeddings(model=\"gemma:2b\"))  ##by default it ues llama2\n",
    "\n",
    "\n",
    "r1=embeddings_ollama.embed_documents([\"Alpha is the first letter of Greek alphabet\", \n",
    "                               \"Beta is the second letter of Greek alphabet\", ])\n",
    "r1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406520ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ollama.embed_query(\"What is the second letter of Greek alphabet \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895682b5",
   "metadata": {},
   "source": [
    "**3. Huggingface Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text=\"this is atest documents\"\n",
    "query_result=embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text, \"This is not a test document.\"])\n",
    "doc_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679add1",
   "metadata": {},
   "source": [
    "### 4. VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487e71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693dc16",
   "metadata": {},
   "source": [
    "**1. FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "### querying \n",
    "query = \"How does the speaker describe the desired outcome of the war?\"\n",
    "docs = db.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deba2cd",
   "metadata": {},
   "source": [
    "As a Retriever\n",
    "\n",
    "We can also convert the vectorstore into a Retriever class. This allows us to easily use it in other LangChain methods, which largely work with retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.invoke(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c336f51",
   "metadata": {},
   "source": [
    "Similarity Search with score\n",
    "\n",
    "There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909915de",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_score = db.similarity_search_with_score(query)\n",
    "\n",
    "### Saving And Loading\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "new_db = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization = True)\n",
    "docs = new_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2efa73",
   "metadata": {},
   "source": [
    "**2. Chroma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd837c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=Chroma.from_documents(documents=splits,embedding=embedding,persist_directory=\"./chroma_db\")\n",
    "# load from disk\n",
    "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\n",
    "docs=db2.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "## similarity Search With Score\n",
    "docs = vectordb.similarity_search_with_score(query)\n",
    "\n",
    "### Retriever option\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever.invoke(query)[0].page_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
